{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Set Up and Evaluation\n",
        "\n",
        "### COSC 89.30 Final Project\n",
        "\n",
        "#### Authors: Tai Wan Kim, Phuc Tran, Mark Lekina Rorat\n",
        "\n",
        "Modified https://github.com/karreny/telling-left-from-right for Google Colab set up and resolved errors/dependency issues.\n",
        "\n",
        "Additional methods implemented to add noise to test set.\n",
        "\n",
        "Reference: K. Yang, B. Russell and J. Salamon, \"Telling Left from Right: Learning Spatial Correspondence between Sight and Sound\", IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Virtual Conference, June 2020."
      ],
      "metadata": {
        "id": "-fBxa-ALTeuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone github repo"
      ],
      "metadata": {
        "id": "2cHxaEr4U86h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZeGitqUTRBb",
        "outputId": "1d950543-f042-4166-e0bf-587df3d7a63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'telling-left-from-right'...\n",
            "remote: Enumerating objects: 253, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 253 (delta 43), reused 105 (delta 32), pack-reused 135\u001b[K\n",
            "Receiving objects: 100% (253/253), 80.83 MiB | 45.25 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karreny/telling-left-from-right.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd telling-left-from-right/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYqLwYCQTUfC",
        "outputId": "2a8d9297-f141-41dd-ebb0-4c7892d7c04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/telling-left-from-right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch upmixing-demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s7S85aOTo7R",
        "outputId": "f3bb00f2-79d4-455f-c652-6f2a94c46d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'upmixing-demo' set up to track remote branch 'upmixing-demo' from 'origin'.\n",
            "Switched to a new branch 'upmixing-demo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip and preprocess downloaded videos"
      ],
      "metadata": {
        "id": "sE6kGf84VHZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/telling-left-from-right/upmixing-final\")\n",
        "\n",
        "from unet import UpmixResnet18Scratch\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "from PIL import Image\n",
        "from IPython.display import Video\n",
        "from IPython.display import Audio\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "CmWqbkpXVzoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RbxzRSySTup2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip folder and create a list of file names for video"
      ],
      "metadata": {
        "id": "Y1ZhtuTGVVA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/89/test_300.zip'\n",
        "destination_folder = '/content/test_300'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)"
      ],
      "metadata": {
        "id": "MuputFJHL6m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/test_300/content/telling-left-from-right/test_data/data/test_300/video/'\n",
        "video_file_names = []\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        video_file_names.append(os.path.splitext(os.path.basename(file_name))[0])\n",
        "\n",
        "print(f\"{len(video_file_names)} videos appended to list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7CMAjiN3wO",
        "outputId": "56e1cb60-09a2-4421-8449-16106cae2f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "293 videos appended to list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess video/audio and save in `preprocessed` folder"
      ],
      "metadata": {
        "id": "qS6ly-awVfvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "SAVE_DIR = \"preprocessed\"\n",
        "\n",
        "if not os.path.isdir(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "def preprocess_video(VIDEO_PATH):\n",
        "    # preprocess video using ffmpeg\n",
        "    video_input_path = os.path.splitext(os.path.basename(VIDEO_PATH))[0] + \".mp4\"\n",
        "    video_input_path = os.path.join(SAVE_DIR, video_input_path)\n",
        "    cmd = \"ffmpeg -i %s -filter:v fps=fps=30 -strict -2 %s\" % (VIDEO_PATH, video_input_path)\n",
        "    print(\"Running in shell:\", cmd)\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "    # extract audio using ffmpeg\n",
        "    audio_input_path = os.path.splitext(os.path.basename(video_input_path))[0] + \".mp3\"\n",
        "    audio_input_path = os.path.join(SAVE_DIR, audio_input_path)\n",
        "    cmd = \"ffmpeg -i %s -f mp3 -ab 192000 -vn %s\" % (video_input_path, audio_input_path)\n",
        "    print(\"Running in shell:\", cmd)\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "for file_name in video_file_names:\n",
        "    video_path = folder_path + file_name + \".mp4\"\n",
        "    preprocess_video(video_path)"
      ],
      "metadata": {
        "id": "Kq1Sf52CQQUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/preprocessed.zip /content/telling-left-from-right/preprocessed/ # create a zip file of downloaded videos"
      ],
      "metadata": {
        "id": "nYXOcwY5syxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/preprocessed.zip\") # download zipped file to local machine for later use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Fvj6LSw-tHKx",
        "outputId": "e10c5d94-da94-4fdc-e4a7-06d66d94a1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_739d9575-7b77-4b5f-bec0-709a7052d0c8\", \"preprocessed.zip\", 467328487)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If uploading preprocessed videos from local machine, unzip"
      ],
      "metadata": {
        "id": "fW-PmKkKT_cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import zipfile\n",
        "\n",
        "# Unzip zip file\n",
        "zip_path = '/content/preprocessed.zip'\n",
        "destination_path = '/content/preprocessed'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_path)"
      ],
      "metadata": {
        "id": "K3-BEjeSWS6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load video/audio and add noise\n",
        "\n",
        "`random_mask` blacks out a given number of randomly selected frames.\n",
        "\n",
        "`tube_mask` blacks out a given number of consecutive frames from a randomly selected starting point."
      ],
      "metadata": {
        "id": "cZy0881GUffA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_mask(v_tensor, cap_frames, r):\n",
        "    '''Our contribution.'''\n",
        "    img_width = v_tensor.shape[1]\n",
        "    img_height = v_tensor.shape[2]\n",
        "\n",
        "    n = int(cap_frames * r)\n",
        "    random_integers = random.sample(range(0, cap_frames), n)\n",
        "    # print(random_integers)\n",
        "\n",
        "    for frame in random_integers:\n",
        "        v_tensor[frame, :, :, :] = np.zeros((1, img_width, img_height, 3))\n",
        "        # print(f\"add noise to frame {frame}\")\n",
        "    \n",
        "    # write_video(v_tensor, img_width, img_height) # uncomment this line to generate video with noise\n",
        "\n",
        "def tube_mask(v_tensor, cap_frames, r):\n",
        "    '''Our contribution.'''\n",
        "    img_width = v_tensor.shape[1]\n",
        "    img_height = v_tensor.shape[2]\n",
        "    \n",
        "    n = int(cap_frames * r)\n",
        "    start_frame = random.sample(range(0, cap_frames - n), 1)[0]\n",
        "    end_frame = start_frame + n\n",
        "\n",
        "    # print(f\"mask out {n} consecutive frames starting at frame {start_frame} and ending at {end_frame}.\")\n",
        "\n",
        "    for frame in range(start_frame, end_frame):\n",
        "        v_tensor[frame, :, :, :] = np.zeros((1, img_width, img_height, 3))\n",
        "    \n",
        "    # write_video(v_tensor, img_width, img_height) # uncomment this line to generate video with noise\n",
        "\n",
        "def rgb_perturb(v_tensor, cap_frames, r):\n",
        "    '''Our contribution.'''\n",
        "\n",
        "    epsilon = 0.2\n",
        "    img_width = v_tensor.shape[1]\n",
        "    img_height = v_tensor.shape[2]\n",
        "\n",
        "    n = int(cap_frames * r)\n",
        "    random_integers = random.sample(range(0, cap_frames), n)\n",
        "\n",
        "    for frame in random_integers:\n",
        "        noise = np.random.uniform(size = (1, img_width, img_height, 3), low= -1* epsilon, high = epsilon)\n",
        "        v_tensor[frame, :, :, :] = np.add(v_tensor[frame, :, :, :], noise)\n",
        "        v_tensor[frame, :, :, :] = np.clip(v_tensor[frame, :, :, :], 0, 255)\n",
        "\n",
        "def write_video(v_tensor, img_width, img_height):\n",
        "    '''Our contribution.'''\n",
        "    # Define the output video file name\n",
        "    output_file = 'noisy_asmr_demo.mp4'\n",
        "\n",
        "    # Create a VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    fps = 30\n",
        "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (img_width, img_height))\n",
        "\n",
        "    # Iterate over each frame in the video array and write it to the video file\n",
        "    for frame in v_tensor:\n",
        "        frame = frame * 255\n",
        "        frame = frame.astype(np.uint8)\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the VideoWriter\n",
        "    video_writer.release()"
      ],
      "metadata": {
        "id": "dBfu8mCMUHso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`load_video` and `load_audio` return numpy array representation of the video and audio."
      ],
      "metadata": {
        "id": "i0sNM48iYMm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import cv2\n",
        "\n",
        "def load_video(videofile, mask_ratio, noise):\n",
        "    '''Modified'''\n",
        "    capture = cv2.VideoCapture(videofile)\n",
        "    cap_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) # total number of frames in the video\n",
        "\n",
        "    v_tensor = []\n",
        "\n",
        "    for idx in range(cap_frames):\n",
        "        ret, frame = capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        v_tensor += [frame]\n",
        "\n",
        "    v_tensor = [Image.fromarray(np.uint8(frame)).resize((224,224)) for frame in v_tensor] # convert numpy array to PIL Image, resized to 224x224\n",
        "    v_tensor = np.stack(v_tensor)/255 # stacks the list of PIL images, normalize to [0,1]\n",
        "\n",
        "    if noise: \n",
        "        # print(\"add random noise to video\")\n",
        "        # random_mask(v_tensor, cap_frames, mask_ratio)\n",
        "        tube_mask(v_tensor, cap_frames, mask_ratio)\n",
        "\n",
        "    return v_tensor, cap_frames\n",
        "\n",
        "def load_audio(audiofile):\n",
        "    '''Modified'''\n",
        "    # resample audio to a sampling rate of 16000 Hz\n",
        "    # If the input audio has multiple channels and mono=True is specified, \n",
        "    # librosa.load() will take the mean across all channels to produce a mono audio signal.\n",
        "    # To preserve the separate channels, set mono=False\n",
        "    audio, _ = librosa.load(audiofile, mono=False, sr=16000)\n",
        "    audio = audio/np.max(np.abs(audio))\n",
        "\n",
        "    if len(audio.shape) == 1: # If the input audio is a single-channel audio, it is duplicated to form a two-channel audio array. \n",
        "        audio = np.stack((audio, audio), axis=0)\n",
        "\n",
        "    # Return audio as a numpy array with shape (2, num_samples), where num_samples is the number of audio samples in the loaded audio file.\n",
        "    # The number of audio samples in a digital audio file is simply the total number of amplitude values that make up the audio signal.\n",
        "    \n",
        "    # print(f\"load audio of shape {audio.shape}\")\n",
        "\n",
        "    return audio\n"
      ],
      "metadata": {
        "id": "_-XjvF6CXnLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clip generator\n",
        "\n",
        "Clip generator generates clips of video and corresponding audio for a given video and audio file."
      ],
      "metadata": {
        "id": "pIZ_aczcWCMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Generate clips of video and corresponding audio for a given video and audio file.'''\n",
        "class ClipGenerator(object):\n",
        "    def __init__(self, video_fps=30, video_downsample_factor=5, audio_sr=16000, clip_length=2.87, hop_length=2):\n",
        "        self.video_fps = video_fps\n",
        "        self.video_downsample_factor = video_downsample_factor\n",
        "        self.audio_sr = audio_sr\n",
        "        self.clip_length = clip_length\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "        self.n_video_frames = int(video_fps*clip_length)\n",
        "        self.n_audio_samples = int(audio_sr*clip_length)\n",
        "        \n",
        "        self.n_video_frames_hop = int(video_fps*hop_length)\n",
        "\n",
        "    def generator(self, videofile, audiofile, mask_ratio, noise=True):\n",
        "        video, total_frames = load_video(videofile, mask_ratio, noise)\n",
        "        audio = load_audio(audiofile)\n",
        "\n",
        "        start_idx = 0\n",
        "\n",
        "        while start_idx < total_frames - self.n_video_frames:\n",
        "            yield self.get_clip(video, audio, start_idx)\n",
        "            start_idx += self.n_video_frames_hop\n",
        "            \n",
        "        yield self.get_clip(video, audio, total_frames - self.n_video_frames, True)\n",
        "\n",
        "    '''\n",
        "    INPUT\n",
        "    video: numpy array of video frames\n",
        "    audio: numpy array of audio samples\n",
        "    start_idx: starting video frame index for the clip\n",
        "    last_clip: whether this is the last clip in the video\n",
        "\n",
        "    OUTPUT\n",
        "    dictionary {\n",
        "      'start_frame': index of start video frame,\n",
        "      'end_frame': index of end video frame,\n",
        "      'start_audio_frame': index of start audio frame,\n",
        "      'end_audio_frame': index of end audio frame,\n",
        "      'video': video tensor,\n",
        "      'audio': audio tensor, \n",
        "      'audio_sum_spec': spectogram for the sum of the two audio channels,\n",
        "      'audio_diff_spec': spectogram for the diff of the two audio channels. (FxT) where T is the number of timesteps and F is the number of frequency bins\n",
        "    }\n",
        "    '''\n",
        "    def get_clip(self, video, audio, start_idx, last_clip=False):\n",
        "        '''Modified'''\n",
        "        clip = {}\n",
        "\n",
        "        videoclip = video[start_idx : start_idx+self.n_video_frames : self.video_downsample_factor]\n",
        "        videoclip = torch.from_numpy(videoclip).float()\n",
        "        videoclip = videoclip.permute(3,0,1,2)\n",
        "        \n",
        "        # print(f\"videoclip: {videoclip}\")\n",
        "        \n",
        "        if last_clip:\n",
        "            audio_start_idx = audio.shape[1]-self.n_audio_samples\n",
        "        else:\n",
        "            audio_start_idx = int(start_idx*self.audio_sr/self.video_fps)\n",
        "            \n",
        "        audioclip = audio[:, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        \n",
        "        left = audio[0, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        right = audio[1, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        mono = (left + right)/2\n",
        "        monoclip = np.stack((mono, mono), axis=0)\n",
        "\n",
        "        audio_sum = audioclip[0] + audioclip[1]\n",
        "        audio_sum_spec = self._get_stft(audio_sum)\n",
        "        audio_sum_spec = torch.from_numpy(audio_sum_spec).float().permute(0,2,1)\n",
        "\n",
        "        audio_diff = audioclip[0] - audioclip[1]\n",
        "        audio_diff_spec = self._get_stft(audio_diff)\n",
        "        audio_diff_spec = torch.from_numpy(audio_diff_spec).float().permute(0,2,1)\n",
        "\n",
        "        return {'start_frame': start_idx, \n",
        "                'end_frame': start_idx+self.n_video_frames, \n",
        "                'start_audio_frame': audio_start_idx, \n",
        "                'end_audio_frame': audio_start_idx+self.n_audio_samples,\n",
        "                'video': videoclip.unsqueeze(0), \n",
        "                'audio': monoclip, \n",
        "                'audio_sum_spec': audio_sum_spec.unsqueeze(0),\n",
        "                'audio_diff_spec': audio_diff_spec.unsqueeze(0)}\n",
        "\n",
        "    '''returns a complex-valued spectrogram in the form of a numpy array.'''\n",
        "    def _get_stft(self, raw):\n",
        "        stft = librosa.core.stft(np.ascontiguousarray(raw), n_fft=512, hop_length=160, win_length=400, center=True)\n",
        "        return np.stack((np.real(stft), np.imag(stft)))[:,:-1,:]\n",
        "\n",
        "    '''converts a complex-valued spectrogram to a raw audio'''\n",
        "    def stft_to_waveform(self, stft):\n",
        "        stft = stft[0,:,:] + (1j * stft[1,:,:])\n",
        "        raw = librosa.core.istft(stft, hop_length=160, win_length=400, center=True)\n",
        "        return raw\n",
        "\n",
        "clip_generator = ClipGenerator()\n"
      ],
      "metadata": {
        "id": "whAzcbmhlUka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load trained model with weights and evaluate on test set\n",
        "\n",
        "The model `UpmixResnet18Scratch` is defined in `telling-left-from-right/upmixing-final/unet.py`. \n",
        "\n",
        "Model weights: https://drive.google.com/file/d/1IDogguisx25enBhikwjbNT0UZaTzpAWN/view?usp=share_link "
      ],
      "metadata": {
        "id": "BGOfpycnZcX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available()) # check GPU status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ7tEhZn-284",
        "outputId": "35e44f8b-0f21-4bf2-a4fe-290d77b67a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "local_path = r'/content/drive/MyDrive/89/upmixing-final-exp-1-flip-checkpoint-best.pth.tar' # upload model checkpoint from google drive\n",
        "\n",
        "# load model\n",
        "model = UpmixResnet18Scratch()\n",
        "\n",
        "pretrained = (local_path)\n",
        "avnet = nn.DataParallel(model)\n",
        "checkpoint = torch.load(pretrained)\n",
        "avnet.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"loaded pretrained model from\", pretrained)\n",
        "model = avnet.module\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "1KD5mVilyyu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### upmix audio and compute loss\n",
        "'''Modified'''\n",
        "\n",
        "import torch.nn as nn\n",
        "import soundfile as sf\n",
        "from scipy.io import wavfile\n",
        "\n",
        "mask_ratio = 0.1 # set r between 0 and 1 (e.g., r = 0.1 means masking out 10% of the frames)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "v_count = 0\n",
        "\n",
        "total_loss = 0\n",
        "n_count = 0\n",
        "\n",
        "for file_name in video_file_names:\n",
        "    # print(f\"{file_name}\")\n",
        "    video_input_path = \"/content/preprocessed/content/telling-left-from-right/preprocessed/\" + file_name + \".mp4\"\n",
        "    audio_input_path = \"/content/preprocessed/content/telling-left-from-right/preprocessed/\" + file_name + \".mp3\"\n",
        "\n",
        "    loader = clip_generator.generator(video_input_path, audio_input_path, mask_ratio, noise=True)\n",
        "\n",
        "    for sample in loader:\n",
        "      keys = model.keys + ['audio_diff_spec']\n",
        "      vars = {k: sample[k] for k in keys}\n",
        "      vars = {k: vars[k].cuda() for k in keys}\n",
        "\n",
        "      out = model(vars)\n",
        "      loss = criterion(out['pred'], vars['audio_diff_spec'])\n",
        "\n",
        "      start_frame = sample['start_frame']\n",
        "      end_frame = sample['end_frame']\n",
        "      start_audio_frame = sample['start_audio_frame']\n",
        "      end_audio_frame = sample['end_audio_frame']\n",
        "\n",
        "      # print(f\"start_frame: {start_frame}, end_frame: {end_frame}, start_audio_frame: {start_audio_frame}, end_audio_frame: {end_audio_frame}, loss: {loss}\")\n",
        "      \n",
        "      total_loss += loss.item()\n",
        "      n_count += 1\n",
        "\n",
        "      # free GPU memory usage\n",
        "      for k in keys:\n",
        "        vars[k] = vars[k].cpu()\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    v_count += 1\n",
        "\n",
        "print(\"===================================================\")\n",
        "print(f\"Masking ratio: {mask_ratio}\")\n",
        "print(f\"Evaluation on {v_count} videos / {n_count} clips.\")\n",
        "print(f\"total_loss: {total_loss}\")\n",
        "print(f\"mean_loss: {total_loss/n_count}\")\n"
      ],
      "metadata": {
        "id": "lqB3nKRuAqmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0fe094-073a-4f68-886c-99b863076e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "Masking ratio: 0.1\n",
            "Evaluation on 293 videos / 1465 clips.\n",
            "total_loss: 145.57209618901834\n",
            "mean_loss: 0.09936661855905689\n"
          ]
        }
      ]
    }
  ]
}