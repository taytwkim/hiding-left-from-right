{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rF0gUWTgofMk",
        "b-eW_9u3xL0R",
        "z3eFl0jfx8kj",
        "rIWecKmd-Dmz",
        "XpA2qDmtRJnC",
        "IXwYpH7-RCRy",
        "kflScoArTzh9",
        "cMxrJPqgWZPn",
        "QyJyWZlQeCer"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF9qIGXsnsj4"
      },
      "outputs": [],
      "source": [
        "# Pipeline \n",
        "# Get the downstream upmixing pre-trained weight model\n",
        "# Train that model with 90 unseen advesarial videos (random black-masked videos)\n",
        "# Compare that model with the original pre-trained model to 160 unseen advesarial test videos\n",
        "# Compare that model with the original pre-trained model to the 48 unseen adversarial tests videos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model Set Up and Evaluation\n",
        "## COSC 89.30 Final Project\n",
        "## Authors: Tai Wan Kim, Phuc Tran, Mark Lekina Rorat\n",
        "Modified https://github.com/karreny/telling-left-from-right for Google Colab set up and resolved errors/dependency issues. Additional adversarial training was done on black-box randomly masked videos. More details are explained in our paper.\n",
        "\n",
        "Pipeline: \n",
        "1. Get the downstream upmixing pre-trained weight model\n",
        "2. Train that model with 90 unseen advesarial videos (random black-masked videos)\n",
        "3. Compare that model with the original pre-trained model to 160 unseen advesarial test videos\n",
        "4. Compare that model with the original pre-trained model to the 48 unseen adversarial tests videos\n",
        "\n",
        "## Additional methods implemented to add noise to test set.\n",
        "\n",
        "## Reference: K. Yang, B. Russell and J. Salamon, \"Telling Left from Right: Learning Spatial Correspondence between Sight and Sound\", IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Virtual Conference, June 2020."
      ],
      "metadata": {
        "id": "SyzM3WmQtdO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basics"
      ],
      "metadata": {
        "id": "rF0gUWTgofMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karreny/telling-left-from-right.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAIHSgdXoid3",
        "outputId": "66cd24fb-ff9f-4b69-bdf5-7a096fbf5648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'telling-left-from-right' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd telling-left-from-right/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wxlhr14wwqqb",
        "outputId": "ea3c3348-4ebf-4827-bb09-dbcf1e351c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/telling-left-from-right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch upmixing-demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XEsUo4gwsno",
        "outputId": "5b1373bf-3712-49ca-b56e-4f5c1f75f211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'upmixing-demo'\n",
            "Your branch is up to date with 'origin/upmixing-demo'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Dependencies"
      ],
      "metadata": {
        "id": "EFn6gyeJwvOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/telling-left-from-right/upmixing-final\")\n",
        "from unet import UpmixResnet18Scratch\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "from PIL import Image\n",
        "from IPython.display import Video\n",
        "from IPython.display import Audio\n",
        "import subprocess\n",
        "import random\n",
        "import cv2\n",
        "### upmix audio and compute loss\n",
        "import torch.nn as nn\n",
        "import soundfile as sf\n",
        "from scipy.io import wavfile"
      ],
      "metadata": {
        "id": "qUsYgMAiw09B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Google Drive to Mount "
      ],
      "metadata": {
        "id": "adAM4hbNw5u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2GlYG4Dw5L4",
        "outputId": "e4971dbc-e974-4949-c3bc-6774710692e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the 300 Videos\n",
        "\n",
        "These 300 videos will be used as a comparative analysis between the adversarial learned weights and the original weights for standard test losses"
      ],
      "metadata": {
        "id": "b-eW_9u3xL0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/COSC89/test_300.zip'\n",
        "destination_folder = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)"
      ],
      "metadata": {
        "id": "QaGZ2etaxKQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the fine tune videos"
      ],
      "metadata": {
        "id": "z3eFl0jfx8kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/COSC89/fine_tune.zip'\n",
        "destination_folder = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)"
      ],
      "metadata": {
        "id": "lQkWQ_Sv97JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPORTANT: ONLY NECESSARY THE FIRST TIME\n",
        "Preprocess the fine-tune videos, the test_300 videos are already preprocessed and put into the Google Drive"
      ],
      "metadata": {
        "id": "rIWecKmd-Dmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video(VIDEO_PATH, SAVE_DIR):\n",
        "    # preprocess video using ffmpeg\n",
        "    video_input_path = os.path.splitext(os.path.basename(VIDEO_PATH))[0] + \".mp4\"\n",
        "    video_input_path = os.path.join(SAVE_DIR, video_input_path)\n",
        "    cmd = \"ffmpeg -i %s -filter:v fps=fps=30 -strict -2 %s\" % (VIDEO_PATH, video_input_path)\n",
        "    print(\"Running in shell:\", cmd)\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "    # extract audio using ffmpeg\n",
        "    audio_input_path = os.path.splitext(os.path.basename(video_input_path))[0] + \".mp3\"\n",
        "    audio_input_path = os.path.join(SAVE_DIR, audio_input_path)\n",
        "    cmd = \"ffmpeg -i %s -f mp3 -ab 192000 -vn %s\" % (video_input_path, audio_input_path)\n",
        "    print(\"Running in shell:\", cmd)\n",
        "    subprocess.call(cmd, shell=True)"
      ],
      "metadata": {
        "id": "RD0EAUl5-Aj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "folder_path = '/content/fine_tune/video/'\n",
        "video_file_names = []\n",
        "\n",
        "SAVE_DIR = \"fine_tune_processed\"\n",
        "os.makedirs(SAVE_DIR + str(\"_train\"))\n",
        "os.makedirs(SAVE_DIR + str(\"_test\"))\n",
        "\n",
        "# if not os.path.isdir(SAVE_DIR):\n",
        "#     os.makedirs(SAVE_DIR)\n",
        "    \n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        video_file_names.append(os.path.splitext(os.path.basename(file_name))[0])\n",
        "\n",
        "print(f\"{len(video_file_names)} videos appended to list.\")\n",
        "\n",
        "# Uncomment to save for the video file to process\n",
        "for i, file_name in enumerate(video_file_names):\n",
        "  video_path = folder_path + file_name + \".mp4\"\n",
        "  cur_save_dir = \"\"\n",
        "  if i < 80: \n",
        "    cur_save_dir = SAVE_DIR + str(\"_train\")\n",
        "  else:\n",
        "    cur_save_dir = SAVE_DIR + str(\"_test\")\n",
        "  preprocess_video(video_path, cur_save_dir)"
      ],
      "metadata": {
        "id": "f5U4q42pG5X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download for fine_tune zip folder for later"
      ],
      "metadata": {
        "id": "DyrfC7BpBfEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the Zip File"
      ],
      "metadata": {
        "id": "65FeEVvUQt4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a zip file of downloaded videos\n",
        "!zip -r /content/fine_tune_test.zip /content/telling-left-from-right/fine_tune_processed_test"
      ],
      "metadata": {
        "id": "J0EMt6PuBYK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/fine_tune_train.zip /content/telling-left-from-right/fine_tune_processed_train"
      ],
      "metadata": {
        "id": "K3Tzcs6vJ-7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/fine_tune_train.zip\") # download zipped file to loc\n",
        "files.download(\"/content/fine_tune_test.zip\") # download zipped file to loc"
      ],
      "metadata": {
        "id": "IAZYSGnyBu4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract the fine_tune_train and fine_tune_test dataset"
      ],
      "metadata": {
        "id": "XpA2qDmtRJnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/COSC89/fine_tune_train.zip'\n",
        "destination_folder = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/COSC89/fine_tune_test.zip'\n",
        "destination_folder = '/content/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)"
      ],
      "metadata": {
        "id": "POJt2xhHQ8pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Up Two Pre-Trained Weights Model-- One will be Trained for Black-Boxed Attacks"
      ],
      "metadata": {
        "id": "IXwYpH7-RCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available()) # check GPU status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfDZz_WxR0u6",
        "outputId": "4ef079a6-cf29-4c31-bb90-85aa5c9140c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "local_path = '/content/drive/MyDrive/COSC89/upmixing-final-exp-1-flip-checkpoint-best.pth.tar' # upload model checkpoint from google drive\n",
        "\n",
        "# load model\n",
        "model = UpmixResnet18Scratch()\n",
        "\n",
        "pretrained = (local_path)\n",
        "avnet = nn.DataParallel(model)\n",
        "checkpoint = torch.load(pretrained)\n",
        "avnet.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"loaded pretrained model from\", pretrained)\n",
        "pre_model = avnet.module\n",
        "pre_model.cuda()"
      ],
      "metadata": {
        "id": "buzjOa8qSHPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "local_path = '/content/drive/MyDrive/COSC89/upmixing-final-exp-1-flip-checkpoint-best.pth.tar' # upload model checkpoint from google drive\n",
        "\n",
        "# load model\n",
        "model = UpmixResnet18Scratch()\n",
        "pretrained = (local_path)\n",
        "avnet = nn.DataParallel(model)\n",
        "checkpoint = torch.load(pretrained)\n",
        "avnet.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"loaded pretrained model from\", pretrained)\n",
        "adv_model = avnet.module\n",
        "adv_model.cuda()"
      ],
      "metadata": {
        "id": "5QK-6jyrSzpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminary Stuffs to Process Stereo to Mono and Add Noise"
      ],
      "metadata": {
        "id": "kflScoArTzh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clip generator\n",
        "\n",
        "Clip generator generates clips of video and corresponding audio for a given video and audio file."
      ],
      "metadata": {
        "id": "wTmC2FUeUhka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Generate clips of video and corresponding audio for a given video and audio file.'''\n",
        "class ClipGenerator(object):\n",
        "    def __init__(self, video_fps=30, video_downsample_factor=5, audio_sr=16000, clip_length=2.87, hop_length=2):\n",
        "        self.video_fps = video_fps\n",
        "        self.video_downsample_factor = video_downsample_factor\n",
        "        self.audio_sr = audio_sr\n",
        "        self.clip_length = clip_length\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "        self.n_video_frames = int(video_fps*clip_length)\n",
        "        self.n_audio_samples = int(audio_sr*clip_length)\n",
        "        \n",
        "        self.n_video_frames_hop = int(video_fps*hop_length)\n",
        "\n",
        "    def generator(self, videofile, audiofile, mask_ratio, noise=True):\n",
        "        video, total_frames = load_video(videofile, mask_ratio, noise)\n",
        "        audio = load_audio(audiofile)\n",
        "\n",
        "        start_idx = 0\n",
        " \n",
        "\n",
        "        while start_idx < total_frames - self.n_video_frames:\n",
        "            yield self.get_clip(video, audio, start_idx)\n",
        "            start_idx += self.n_video_frames_hop\n",
        "            \n",
        "        yield self.get_clip(video, audio, total_frames - self.n_video_frames, True)\n",
        "\n",
        "    '''\n",
        "    INPUT\n",
        "    video: numpy array of video frames\n",
        "    audio: numpy array of audio samples\n",
        "    start_idx: starting video frame index for the clip\n",
        "    last_clip: whether this is the last clip in the video\n",
        "\n",
        "    OUTPUT\n",
        "    dictionary {\n",
        "      'start_frame': index of start video frame,\n",
        "      'end_frame': index of end video frame,\n",
        "      'start_audio_frame': index of start audio frame,\n",
        "      'end_audio_frame': index of end audio frame,\n",
        "      'video': video tensor,\n",
        "      'audio': audio tensor, \n",
        "      'audio_sum_spec': spectogram for the sum of the two audio channels,\n",
        "      'audio_diff_spec': spectogram for the diff of the two audio channels. (FxT) where T is the number of timesteps and F is the number of frequency bins\n",
        "    }\n",
        "    '''\n",
        "    def get_clip(self, video, audio, start_idx, last_clip=False):\n",
        "        clip = {}\n",
        "\n",
        "        videoclip = video[start_idx : start_idx+self.n_video_frames : self.video_downsample_factor]\n",
        "        videoclip = torch.from_numpy(videoclip).float()\n",
        "        videoclip = videoclip.permute(3,0,1,2)\n",
        "                \n",
        "        if last_clip:\n",
        "            audio_start_idx = audio.shape[1]-self.n_audio_samples\n",
        "        else:\n",
        "            audio_start_idx = int(start_idx*self.audio_sr/self.video_fps)\n",
        "            \n",
        "        audioclip = audio[:, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        \n",
        "        left = audio[0, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        right = audio[1, audio_start_idx : audio_start_idx+self.n_audio_samples]\n",
        "        mono = (left + right)/2\n",
        "        monoclip = np.stack((mono, mono), axis=0)\n",
        "\n",
        "        audio_sum = audioclip[0] + audioclip[1]\n",
        "        audio_sum_spec = self._get_stft(audio_sum)\n",
        "        audio_sum_spec = torch.from_numpy(audio_sum_spec).float().permute(0,2,1)\n",
        "\n",
        "        audio_diff = audioclip[0] - audioclip[1]\n",
        "        audio_diff_spec = self._get_stft(audio_diff)\n",
        "        audio_diff_spec = torch.from_numpy(audio_diff_spec).float().permute(0,2,1)\n",
        "\n",
        "        return {'start_frame': start_idx, \n",
        "                'end_frame': start_idx+self.n_video_frames, \n",
        "                'start_audio_frame': audio_start_idx, \n",
        "                'end_audio_frame': audio_start_idx+self.n_audio_samples,\n",
        "                'video': videoclip.unsqueeze(0), \n",
        "                'audio': monoclip, \n",
        "                'audio_sum_spec': audio_sum_spec.unsqueeze(0),\n",
        "                'audio_diff_spec': audio_diff_spec.unsqueeze(0)}\n",
        "\n",
        "    '''returns a complex-valued spectrogram in the form of a numpy array.'''\n",
        "    def _get_stft(self, raw):\n",
        "        stft = librosa.core.stft(np.ascontiguousarray(raw), n_fft=512, hop_length=160, win_length=400, center=True)\n",
        "        return np.stack((np.real(stft), np.imag(stft)))[:,:-1,:]\n",
        "\n",
        "    '''converts a complex-valued spectrogram to a raw audio'''\n",
        "    def stft_to_waveform(self, stft):\n",
        "        stft = stft[0,:,:] + (1j * stft[1,:,:])\n",
        "        raw = librosa.core.istft(stft, hop_length=160, win_length=400, center=True)\n",
        "        return raw"
      ],
      "metadata": {
        "id": "ezLO8inOT2ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading video as a tensor and adding noise"
      ],
      "metadata": {
        "id": "AkqnDxygWO4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(videofile, mask_ratio, noise):\n",
        "    capture = cv2.VideoCapture(videofile)\n",
        "    cap_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) # total number of frames in the video\n",
        "\n",
        "    v_tensor = []\n",
        "\n",
        "    for idx in range(cap_frames):\n",
        "        ret, frame = capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        v_tensor += [frame]\n",
        "\n",
        "    v_tensor = [Image.fromarray(np.uint8(frame)).resize((224,224)) for frame in v_tensor] # convert numpy array to PIL Image, resized to 224x224\n",
        "    v_tensor = np.stack(v_tensor)/255 # stacks the list of PIL images, normalize to [0,1]\n",
        "    if noise: \n",
        "        # add random noise to video\n",
        "        add_noise(v_tensor, cap_frames, mask_ratio)\n",
        "\n",
        "    return v_tensor, cap_frames\n",
        "\n",
        "'''set r between 0 and 1 (e.g., r=0.1 means masking out 10% of the frames)'''\n",
        "def add_noise(v_tensor, cap_frames, r):\n",
        "    '''Our contribution.'''\n",
        "    epsilon = 0.2\n",
        "    img_width = v_tensor.shape[1]\n",
        "    img_height = v_tensor.shape[2]\n",
        "\n",
        "    n = int(cap_frames * r) # number of frames to mask out\n",
        "    random_integers = random.sample(range(0, cap_frames), n) # randomly sample frames to mask out\n",
        "\n",
        "    for frame in random_integers:\n",
        "        '''\n",
        "        # adding noise \n",
        "        noise = np.random.uniform(size = (1, img_width, img_height, 3), low= -1* epsilon, high = epsilon)\n",
        "        v_tensor[frame, :, :, :] = np.add(v_tensor[frame, :, :, :], noise)\n",
        "        v_tensor[frame, :, :, :] = np.clip(v_tensor[frame, :, :, :], 0, 255)\n",
        "        '''\n",
        "        # masked non-sequential\n",
        "        v_tensor[frame, :, :, :] = np.zeros((1, img_width, img_height, 3))\n",
        "    # Uncomment below lines to generate a sample of masked video\n",
        "    '''\n",
        "    # Define the output video file name\n",
        "    output_file = '/content/DEMOHERE.mp4'\n",
        "\n",
        "    #Create a VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    fps = 30\n",
        "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (img_width, img_height))\n",
        "\n",
        "    # Iterate over each frame in the video array and write it to the video file\n",
        "    for frame in v_tensor:\n",
        "        frame = frame * 255\n",
        "        frame = frame.astype(np.uint8)\n",
        "        video_writer.write(frame)\n",
        "\n",
        "    # Release the VideoWriter\n",
        "    video_writer.release()\n",
        "    '''\n",
        "\n",
        "def load_audio(audiofile):\n",
        "    # resample audio to a sampling rate of 16000 Hz\n",
        "    # If the input audio has multiple channels and mono=True is specified, \n",
        "    # librosa.load() will take the mean across all channels to produce a mono audio signal.\n",
        "    # To preserve the separate channels, set mono=False\n",
        "    # print(\"audio\")\n",
        "    audio, _ = librosa.load(audiofile, mono=False, sr=16000)\n",
        "    audio = audio/np.max(np.abs(audio))\n",
        "\n",
        "    if len(audio.shape) == 1: # If the input audio is a single-channel audio, it is duplicated to form a two-channel audio array. \n",
        "        audio = np.stack((audio, audio), axis=0)\n",
        "\n",
        "    # Return audio as a numpy array with shape (2, num_samples), where num_samples is the number of audio samples in the loaded audio file.\n",
        "    # The number of audio samples in a digital audio file is simply the total number of amplitude values that make up the audio signal.\n",
        "    \n",
        "    # print(f\"load audio of shape {audio.shape}\")\n",
        "\n",
        "    return audio"
      ],
      "metadata": {
        "id": "0wtZYrNWUmKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "cMxrJPqgWZPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'content/telling-left-from-right/fine_tune_processed_train/'\n",
        "video_file_names = []\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        video_file_names.append(os.path.splitext(os.path.basename(file_name))[0])\n",
        "\n",
        "print(f\"{len(video_file_names)} videos appended to list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSe7dAw8Woln",
        "outputId": "00b0e28e-d3bd-4dfb-cfc8-70956de21068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160 videos appended to list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train!"
      ],
      "metadata": {
        "id": "LCmZxIq0Xrcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization \n",
        "mask_ratio = 0.5 # set r between 0 and 1 (e.g., r=0.1 means masking out 10% of the frames)\n",
        "criterion = nn.L1Loss()\n",
        "v_count = 0\n",
        "total_loss = 0\n",
        "n_count = 0\n",
        "clip_generator = ClipGenerator()\n",
        "adv_model.train()\n",
        "optimizer = torch.optim.SGD(adv_model.parameters(), lr=1e-3, momentum = 0.9)\n",
        "\n",
        "for file_name in video_file_names:\n",
        "    # print(f\"{file_name}\")\n",
        "    video_input_path = \"content/telling-left-from-right/fine_tune_processed_train/\" + file_name + \".mp4\"\n",
        "    audio_input_path = \"content/telling-left-from-right/fine_tune_processed_train/\" + file_name + \".mp3\"\n",
        "\n",
        "    loader = clip_generator.generator(video_input_path, audio_input_path, mask_ratio, noise=True)\n",
        "    for sample in loader:\n",
        "      keys = adv_model.keys + ['audio_diff_spec']\n",
        "      vars = {k: sample[k] for k in keys}\n",
        "      vars = {k: vars[k].cuda() for k in keys}\n",
        "\n",
        "      # compare the predicted with the ground truth\n",
        "      out = adv_model(vars)\n",
        "      loss = criterion(out['pred'], vars['audio_diff_spec'])\n",
        "\n",
        "      # saves random stuffs\n",
        "      start_frame = sample['start_frame']\n",
        "      end_frame = sample['end_frame']\n",
        "      start_audio_frame = sample['start_audio_frame']\n",
        "      end_audio_frame = sample['end_audio_frame']\n",
        "      \n",
        "      total_loss += loss.item()\n",
        "      n_count += 1\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # free GPU memory usage\n",
        "      for k in keys:\n",
        "        vars[k] = vars[k].cpu()\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    v_count += 1\n",
        "\n",
        "\n",
        "print(\"===================================================\")\n",
        "print(f\"Masking ratio: {mask_ratio}\")\n",
        "print(f\"Evaluation on {v_count} videos / {n_count} clips.\")\n",
        "print(f\"total_loss: {total_loss}\")\n",
        "print(f\"mean_loss: {total_loss/n_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2b7JRixWbWY",
        "outputId": "0f7f5f48-6257-4ebe-c3c1-96d0a9ee4332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "Masking ratio: 0.5\n",
            "Evaluation on 160 videos / 800 clips.\n",
            "total_loss: 107.32094969693571\n",
            "mean_loss: 0.13415118712116963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with the Best-Trained Model and the Adversarial Model"
      ],
      "metadata": {
        "id": "QyJyWZlQeCer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'content/telling-left-from-right/fine_tune_processed_test/'\n",
        "video_file_names = []\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if os.path.isfile(file_path):\n",
        "        video_file_names.append(os.path.splitext(os.path.basename(file_name))[0])\n",
        "\n",
        "print(f\"{len(video_file_names)} videos appended to list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFezEQralFz1",
        "outputId": "2485ccc7-35ae-43fe-80e1-a31a2991a1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 videos appended to list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization \n",
        "mask_ratio = 0.5 # set r between 0 and 1 (e.g., r=0.1 means masking out 10% of the frames)\n",
        "criterion = nn.L1Loss()\n",
        "v_count = 0\n",
        "total_loss = 0\n",
        "n_count = 0\n",
        "clip_generator = ClipGenerator()\n",
        "adv_model.eval()\n",
        "\n",
        "for file_name in video_file_names:\n",
        "    # print(f\"{file_name}\")\n",
        "    video_input_path = \"content/telling-left-from-right/fine_tune_processed_test/\" + file_name + \".mp4\"\n",
        "    audio_input_path = \"content/telling-left-from-right/fine_tune_processed_test/\" + file_name + \".mp3\"\n",
        "\n",
        "    loader = clip_generator.generator(video_input_path, audio_input_path, mask_ratio, noise=True)\n",
        "    for sample in loader:\n",
        "      keys = adv_model.keys + ['audio_diff_spec']\n",
        "      vars = {k: sample[k] for k in keys}\n",
        "      vars = {k: vars[k].cuda() for k in keys}\n",
        "\n",
        "      # compare the predicted with the ground truth\n",
        "      out = adv_model(vars)\n",
        "      loss = criterion(out['pred'], vars['audio_diff_spec'])\n",
        "\n",
        "      # saves random stuffs\n",
        "      start_frame = sample['start_frame']\n",
        "      end_frame = sample['end_frame']\n",
        "      start_audio_frame = sample['start_audio_frame']\n",
        "      end_audio_frame = sample['end_audio_frame']\n",
        "      \n",
        "      total_loss += loss.item()\n",
        "      n_count += 1\n",
        "\n",
        "      # free GPU memory usage\n",
        "      for k in keys:\n",
        "        vars[k] = vars[k].cpu()\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    v_count += 1\n",
        "\n",
        "\n",
        "print(\"===================================================\")\n",
        "print(f\"Masking ratio: {mask_ratio}\")\n",
        "print(f\"Evaluation on {v_count} videos / {n_count} clips.\")\n",
        "print(f\"total_loss: {total_loss}\")\n",
        "print(f\"mean_loss: {total_loss/n_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACqodcQ6Z_49",
        "outputId": "5efe8c2f-5944-495e-c678-e9bd7e853388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "Masking ratio: 0.5\n",
            "Evaluation on 48 videos / 240 clips.\n",
            "total_loss: 33.74488641601056\n",
            "mean_loss: 0.140603693400044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization \n",
        "mask_ratio = 0.5 # set r between 0 and 1 (e.g., r=0.1 means masking out 10% of the frames)\n",
        "criterion = nn.L1Loss()\n",
        "v_count = 0\n",
        "total_loss = 0\n",
        "n_count = 0\n",
        "clip_generator = ClipGenerator()\n",
        "pre_model.eval()\n",
        "\n",
        "for file_name in video_file_names:\n",
        "    # print(f\"{file_name}\")\n",
        "    video_input_path = \"content/telling-left-from-right/fine_tune_processed_test/\" + file_name + \".mp4\"\n",
        "    audio_input_path = \"content/telling-left-from-right/fine_tune_processed_test/\" + file_name + \".mp3\"\n",
        "\n",
        "    loader = clip_generator.generator(video_input_path, audio_input_path, mask_ratio, noise=True)\n",
        "    for sample in loader:\n",
        "      keys = pre_model.keys + ['audio_diff_spec']\n",
        "      vars = {k: sample[k] for k in keys}\n",
        "      vars = {k: vars[k].cuda() for k in keys}\n",
        "\n",
        "      # compare the predicted with the ground truth\n",
        "      out = pre_model(vars)\n",
        "      loss = criterion(out['pred'], vars['audio_diff_spec'])\n",
        "\n",
        "      # saves random stuffs\n",
        "      start_frame = sample['start_frame']\n",
        "      end_frame = sample['end_frame']\n",
        "      start_audio_frame = sample['start_audio_frame']\n",
        "      end_audio_frame = sample['end_audio_frame']\n",
        "      \n",
        "      total_loss += loss.item()\n",
        "      n_count += 1\n",
        "\n",
        "      # free GPU memory usage\n",
        "      for k in keys:\n",
        "        vars[k] = vars[k].cpu()\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    v_count += 1\n",
        "\n",
        "\n",
        "print(\"===================================================\")\n",
        "print(f\"Masking ratio: {mask_ratio}\")\n",
        "print(f\"Evaluation on {v_count} videos / {n_count} clips.\")\n",
        "print(f\"total_loss: {total_loss}\")\n",
        "print(f\"mean_loss: {total_loss/n_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RPk940hg1yP",
        "outputId": "2e309140-e843-4dae-aa6e-ac0a3b842b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "Masking ratio: 0.5\n",
            "Evaluation on 48 videos / 240 clips.\n",
            "total_loss: 33.10120473615825\n",
            "mean_loss: 0.13792168640065938\n"
          ]
        }
      ]
    }
  ]
}